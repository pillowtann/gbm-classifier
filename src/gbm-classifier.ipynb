{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from sklearn.preprocessing import Imputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Reverting Dummies\n",
    "The following two functions helps to convert a dataframe (X_train) with categorical variables (cat_var) and numerical variables (num_var) into a final dataframe that includes both dummies plus numericals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies_into_df(X_train, cat_var, num_var):\n",
    "    X_train_dumm = X_train\n",
    "    \n",
    "    for var in cat_var:\n",
    "        cat_list = 'var'+'_'+var\n",
    "        cat_list = pd.get_dummies(X_train_dumm[var], prefix=var)\n",
    "        X_train_dumm = X_train_dumm.join(cat_list)\n",
    "    \n",
    "    all_var = X_train_dumm.columns.values.tolist()\n",
    "    original_var = X_train.columns.values.tolist()\n",
    "    dummies_list = [i for i in all_var if i not in original_var]\n",
    "    to_keep = num_var + dummies_list\n",
    "    X_train_dumm = X_train_dumm[to_keep]\n",
    "    \n",
    "    return X_train_dumm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_from_x_dummies(df, cat_var, num_var, data = 'not_checking'):\n",
    "    # df: pandas DataFrame with all numericals\n",
    "    # cat_var: list of categorical variables in dummies in df\n",
    "    # num_var: list of numerical variables in df\n",
    "    # data: pandas DataFrame if checking with original converted table\n",
    "    \n",
    "    all_merged_columns = pd.DataFrame()\n",
    "\n",
    "    for ori_col in cat_var:\n",
    "        #print('Performing merge_dummies for:', ori_col)\n",
    "\n",
    "        # Structure of dummies is with _ split\n",
    "        ori_col_ = ori_col + '_'\n",
    "\n",
    "        # Finding dummy columns based on cat_var & num_var\n",
    "        dummy_tuples = [(col.split(ori_col_)[1],col) for col in df.columns if ori_col_ in col if col not in num_var]\n",
    "        dummy_columns = [col for col in df.columns if ori_col_ in col if col not in num_var]\n",
    "        dummy_df = df[dummy_columns]\n",
    "        sum_across = dummy_df.sum(axis=1)\n",
    "\n",
    "        # Get column results\n",
    "        for dummy, cols in groupby(dummy_tuples, lambda item: item[0]):\n",
    "            # Find max value among columns\n",
    "            max_columns = dummy_df.idxmax(axis=1)\n",
    "\n",
    "            # Set results to missing if all dummies are missing\n",
    "            all_merged_columns[ori_col] = sum_across.apply(lambda x: 'missing' if x==0 else np.nan)\n",
    "\n",
    "            # Remove category_ prefix\n",
    "            all_merged_columns[ori_col] = all_merged_columns[ori_col].fillna(max_columns.apply(lambda item: item.split(ori_col_)[1]))\n",
    "\n",
    "        if type(data) == pd.core.frame.DataFrame:\n",
    "            # Check result\n",
    "            # You may ignore checking if using SMOTE\n",
    "            print(list(all_merged_columns[ori_col]) == list(data[ori_col].fillna('missing')))\n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "    data_final = pd.concat([df[num_var], all_merged_columns], axis=1)\n",
    "    \n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE\n",
    "Using oversampling package, SMOTE or BorderlineSMOTE (user may change the code below), function takes X variables (col name: df_train_columns) and y_train (col name: target_var) to perform oversampling.<br>\n",
    "Please note that the X variables must be converted to numericals and have no missing variables. Thus is suggested to use with get_dummies_into_df and Imputer package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling_SMOTE(X_train, y_train, df_train_columns, target_var):\n",
    "    # X_train: pandas DataFrame\n",
    "    # y_train: list\n",
    "    # target_var: str\n",
    "    \n",
    "    # Check the numbers of our data\n",
    "    print(\"Length of original data is \",len(X_train))\n",
    "    print(\"Proportion of AVERAGE sellers in original data\",len(X_train[y_train==0])/len(X_train))\n",
    "    print(\"Proportion of BEST sellers in original data\",len(X_train[y_train==1])/len(X_train))\n",
    "    \n",
    "    # Counter oversampling\n",
    "    os = SMOTE(random_state=0) #or\n",
    "    # os = BorderlineSMOTE(random_state=0)\n",
    "    os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "    del(X_train, y_train)\n",
    "    os_data_X = pd.DataFrame(data=os_data_X,columns=df_train_columns)\n",
    "    os_data_y= pd.DataFrame(data=os_data_y,columns=[target_var])\n",
    "    \n",
    "    print(\"Length of oversampled data is \",len(os_data_X))\n",
    "    print(\"Proportion of AVERAGE sellers data in oversampled data is \",len(os_data_y[os_data_y[target_var]==0])/len(os_data_X))\n",
    "    print(\"Proportion of BEST sellers data in oversampled data is \",len(os_data_y[os_data_y[target_var]==1])/len(os_data_X))\n",
    "    \n",
    "    return os_data_X, os_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Functions\n",
    "The following definitions help to save code lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(predicted, actual):\n",
    "    rmse = ((predicted - actual) ** 2).mean() ** .5\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_half_up(n, decimals=0):\n",
    "    multiplier = 10 ** decimals\n",
    "    return np.floor(n*multiplier + 0.5) / multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "The following function is the actual LightGBM function that wraps all the above codes in a flow highlighted in the README file of this repository. Please refer to the file for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lightgbm_result_best(data, cat_var, num_var, target_var, final_target,\n",
    "                        train_till, fold_by, future = 'unfilled',\n",
    "                        n_splits = 5, asym_val = 1.0, sample_size = 50000,\n",
    "                        params = {'num_leaves' : 31}, metrics = [\"mse\", 'mae']):\n",
    "    \n",
    "    for column in cat_var:\n",
    "        data[column] =  data[column].astype('category')\n",
    "    \n",
    "    if type(future) == str:\n",
    "        pass\n",
    "    else:\n",
    "        for column in cat_var:\n",
    "            future[column] =  future[column].astype('category')\n",
    "        \n",
    "    df_train_columns = cat_var + num_var\n",
    "    \n",
    "    data['FirstSalesDate'] = pd.to_datetime(data['FirstSalesDate'])\n",
    "    test = data[data['FirstSalesDate'] >= pd.to_datetime(train_till)].copy()\n",
    "    train = data[data['FirstSalesDate'] < pd.to_datetime(train_till)].copy()\n",
    "    test['qty_bins'] = pd.qcut(test[final_target].rank(method='first'), 4, \n",
    "                               labels=[\"0-25\", \"25-50\", \"50-75\",\"75-100\"], duplicates='drop')\n",
    "    \n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    if type(future) == str:\n",
    "        pass\n",
    "    else:\n",
    "        forecasts = np.zeros(len(future))\n",
    "    \n",
    "    X = train[df_train_columns]\n",
    "    y = train[target_var]\n",
    "    groups = train[fold_by]\n",
    "    del(train)\n",
    "    \n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    group_kfold.get_n_splits(X, y, groups)\n",
    "\n",
    "    fold_ = 0\n",
    "\n",
    "    for trn_idx, val_idx in group_kfold.split(X, y, groups):\n",
    "        \n",
    "        fold_ = fold_ + 1\n",
    "        print('Model performing Fold', str(fold_))\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Get dummies\n",
    "        X_train_dumm = get_dummies_into_df(X_train, cat_var, num_var)\n",
    "        \n",
    "        # Impute missing values\n",
    "        imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "        X_train_fit = imp.fit_transform(X_train_dumm)\n",
    "        \n",
    "        # SMOTE\n",
    "        X_train, y_train = oversampling_SMOTE(X_train_fit, y_train, X_train_dumm.columns, target_var)\n",
    "        del(X_train_fit, X_train_dumm)\n",
    "    \n",
    "        # Limited space on laptop to run!\n",
    "        # Best if can batch process\n",
    "        if sample_size == 'NULL':\n",
    "            pass\n",
    "        else:\n",
    "            idx = list(np.random.choice(y_train.index.values, sample_size, replace=False))\n",
    "            y_train = y_train.iloc[idx].copy()\n",
    "            X_train = X_train.iloc[idx].copy()\n",
    "            print(\"Proportion of BEST sellers in sampled data is \", \n",
    "                  len(y_train[y_train[target_var]==1])/len(X_train))\n",
    "\n",
    "        # Back to model variables, reset cat\n",
    "        X_train = back_from_x_dummies(X_train, cat_var, num_var, data = 'not_checking')\n",
    "        for column in cat_var:\n",
    "            X_train[column] =  X_train[column].astype('category')\n",
    "        X_train = X_train[df_train_columns]\n",
    "        \n",
    "        # Modelling\n",
    "        gbm = lgb.LGBMRegressor(random_state=33)\n",
    "        \n",
    "        if asym_val == 1.0:\n",
    "            gbm.set_params(**{**{'objective': 'binary'}, **params}, metrics = metrics, silent=False)\n",
    "        else:\n",
    "            def custom_asymmetric_objective(y_true, y_pred):\n",
    "                residual = (y_true - y_pred).astype(\"float\")\n",
    "                grad = np.where(residual>0, -2*asym_val*residual, -2*residual)\n",
    "                hess = np.where(residual>0, 2*asym_val, 2.0)\n",
    "                return grad, hess\n",
    "            gbm.set_params(**{**{'objective': custom_asymmetric_objective}, **params}, metrics = metrics, silent=False)\n",
    "\n",
    "        gbm.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='l2',\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        oof[val_idx] = gbm.predict(X_val)\n",
    "\n",
    "        if target_var[:3]=='log':\n",
    "            val_rmse = ((np.exp(oof[val_idx]) - np.exp(y_val)) ** 2).mean() ** .5\n",
    "        else:\n",
    "            val_rmse = ((oof[val_idx] - y_val) ** 2).mean() ** .5\n",
    "\n",
    "        # predict\n",
    "        predictions = predictions*(fold_-1)\n",
    "        predictions += gbm.predict(test[df_train_columns])\n",
    "        predictions = predictions/ fold_\n",
    "        \n",
    "        # forecast\n",
    "        if type(future) == str:\n",
    "            pass\n",
    "        else:\n",
    "            forecasts = forecasts*(fold_-1)\n",
    "            forecasts += gbm.predict(future[df_train_columns])\n",
    "            forecasts = forecasts/ fold_\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    if target_var[:3]=='log':\n",
    "        test_rmse = get_rmse(np.exp(predictions), np.exp(test[target_var]))\n",
    "        test_rmse_25 = get_rmse(np.exp(predictions[test['qty_bins'] == \"0-25\"]), np.exp(test[test['qty_bins'] == \"0-25\"][target_var]))\n",
    "        test_rmse_50 = get_rmse(np.exp(predictions[test['qty_bins'] == \"25-50\"]), np.exp(test[test['qty_bins'] == \"25-50\"][target_var]))\n",
    "        test_rmse_75 = get_rmse(np.exp(predictions[test['qty_bins'] == \"50-75\"]), np.exp(test[test['qty_bins'] == \"50-75\"][target_var]))\n",
    "        test_rmse_100 = get_rmse(np.exp(predictions[test['qty_bins'] == \"75-100\"]), np.exp(test[test['qty_bins'] == \"75-100\"][target_var]))\n",
    "    else:\n",
    "        test_rmse = get_rmse(predictions, test[target_var])\n",
    "        test_rmse_25 = get_rmse(predictions[test['qty_bins'] == \"0-25\"], test[test['qty_bins'] == \"0-25\"][target_var])\n",
    "        test_rmse_50 = get_rmse(predictions[test['qty_bins'] == \"25-50\"], test[test['qty_bins'] == \"25-50\"][target_var])\n",
    "        test_rmse_75 = get_rmse(predictions[test['qty_bins'] == \"50-75\"], test[test['qty_bins'] == \"50-75\"][target_var])\n",
    "        test_rmse_100 = get_rmse(predictions[test['qty_bins'] == \"75-100\"], test[test['qty_bins'] == \"75-100\"][target_var])\n",
    "    \n",
    "    result.append((cat_var, num_var, target_var, final_target,\n",
    "                   train_till, fold_by, n_splits, asym_val, metrics, \n",
    "                   test_rmse, test_rmse_25, test_rmse_50, test_rmse_75, test_rmse_100))\n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = ['cat_var', 'num_var', 'target_var', 'final_target',\n",
    "                      'train_till', 'fold_by', 'n_splits', 'asym_val', 'metrics',\n",
    "                      'test_rmse', 'test_rmse_25', 'test_rmse_50', 'test_rmse_75', 'test_rmse_100']\n",
    "    \n",
    "    if type(future) == str:\n",
    "        return gbm, predictions, oof, result\n",
    "    else:\n",
    "        return gbm, predictions, oof, result, forecasts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model\n",
    "Now we may run the model by inputing our data, variables, and parameters.\n",
    "The result table helps to do error analysis across another y_variable, in my use case, the actual sold qty. This helps me check where my model misses predictions (Is_Sold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define variables\n",
    "data = data #put dataset here\n",
    "cat_var = ['colour', 'size', 'details'] #list of categorical variables\n",
    "num_var= ['heelheight', 'price'] #list of numerical variables \n",
    "target_var = 'Is_Sold' #str of column name\n",
    "final_target = 'TotalSoldQty' #str of column name for error analysis\n",
    "\n",
    "train_till = '06-01-2019' #str of train-test split by time\n",
    "fold_by = 'article' #str of column name to group fold by\n",
    "future = 'NULL' #str or data for forecasting\n",
    "\n",
    "n_splits = 10 #number of kfold splits run\n",
    "asym_val = 1 #if 1, runs binary objective. else runs asymmetric custom objective\n",
    "sample_size = 50000 #to turn of random sampling, put 'NULL'\n",
    "params = {'num_leaves' : 31} #gbm params, see documentation https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "\n",
    "# Run function\n",
    "gbm, predictions, oof, result, forecasts = get_lightgbm_classifier(\n",
    "    data, cat_var, num_var, target_var, final_target,\n",
    "    train_till, fold_by, future = future,\n",
    "    n_splits = n_splits, asym_val = asym_val, sample_size = sample_size,\n",
    "    params = params, metrics = [\"mse\", 'mae'])\n",
    "\n",
    "# Print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Results\n",
    "Using standard classification error reviewing methods like Precision, Recall, ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[data['FirstSalesDate'] >= pd.to_datetime(train_till)].copy()\n",
    "train = data[data['FirstSalesDate'] < pd.to_datetime(train_till)].copy()\n",
    "\n",
    "train['pred_Is_Sold'] = oof\n",
    "train['pred_Is_Sold'] = train['pred_Is_Sold'].apply(lambda x: round_half_up(x))\n",
    "train['pred_Is_Sold'] = train['pred_Is_Sold'].apply(lambda x: 0 if x<=0 else 1)\n",
    "\n",
    "test['pred_Is_Sold'] = predictions\n",
    "test['pred_Is_Sold'] = test['pred_Is_Sold'].apply(lambda x: round_half_up(x))\n",
    "#test['pred_Is_Sold'] = test['pred_Is_Sold'].apply(lambda x: 0 if x<=0 else 1)\n",
    "\n",
    "future['pred_Is_Sold'] = pd.DataFrame(forecasts).apply(lambda x: round_half_up(x))\n",
    "#future['pred_Is_Sold'] = future['pred_Is_Sold'].apply(lambda x: 0 if x<=0 else 1)\n",
    "\n",
    "pred_data = pd.concat([train, test])\n",
    "pred_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN Confusion Matrix\n",
    "\n",
    "confusion_matrix = pd.crosstab(train['pred_Is_Sold'], train[target_var], rownames=['Pred'], colnames=['Actual'])\n",
    "\n",
    "if len(confusion_matrix)==2:\n",
    "    accuracy_rate = round(((confusion_matrix.iloc[0,0] + confusion_matrix.iloc[1,1])/ (confusion_matrix.iloc[0,1] + confusion_matrix.iloc[1,0] + confusion_matrix.iloc[0,0] + confusion_matrix.iloc[1,1]))*100,2)\n",
    "    print(\"Accuracy:\", accuracy_rate,\"%\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Confusion Matrix\n",
    "\n",
    "confusion_matrix = pd.crosstab(test['pred_Is_Sold'], test[target_var], rownames=['Pred'], colnames=['Actual'])\n",
    "\n",
    "if len(confusion_matrix)==2:\n",
    "    accuracy_rate = round(((confusion_matrix.iloc[0,0] + confusion_matrix.iloc[1,1])/ (confusion_matrix.iloc[0,1] + confusion_matrix.iloc[1,0] + confusion_matrix.iloc[0,0] + confusion_matrix.iloc[1,1]))*100,2)\n",
    "    print(\"Accuracy:\", accuracy_rate,\"%\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and Recall\n",
    "\n",
    "print(classification_report(test['pred_Is_Sold'], test[target_var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "\n",
    "logit_roc_auc = roc_auc_score(test[target_var], gbm.predict(test[cat_var+num_var]))\n",
    "fpr, tpr, thresholds = roc_curve(test[target_var], gbm.predict(test[cat_var+num_var]))\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
